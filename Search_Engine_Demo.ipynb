{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5025bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e01336",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "class TwoImageDataset(Dataset):\n",
    "    \"\"\" Own dataset \"\"\"\n",
    "\n",
    "    def __init__(self, quickdraw_dir, realworld_dir, fruit_category, image_size = 255, class_size = 5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sketch_dir (string): Directory to all the sketch images.\n",
    "            realworld_dir (string): Directory to all the real world images.\n",
    "            fruit_category: list to fruit catogory\n",
    "            class_size: Num of images in each category\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.quickdraw_dir = quickdraw_dir\n",
    "        self.realworld_dir = realworld_dir\n",
    "        self.transform = transform\n",
    "        self.fruit_category = fruit_category\n",
    "        self.class_size = class_size\n",
    "        self.quickdraw_data_dict = dict(np.load(quickdraw_dir))\n",
    "        self.realworld_data_dict = dict(np.load(realworld_dir))\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.transform_img = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                transforms.Resize(image_size),\n",
    "                                                transforms.ToTensor()])\n",
    "        self.transform_label = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.class_size * len(self.fruit_category)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        class_index = int(idx // self.class_size)\n",
    "        category =  self.fruit_category[class_index]\n",
    "        category_idx_real = random.choice(range(self.class_size))\n",
    "        category_idx_sketch = random.choice(range(self.class_size))\n",
    "        \n",
    "        label = np.zeros((len(self.fruit_category), 1))\n",
    "        label[class_index] = 1\n",
    "      \n",
    "        quickdraw_ary =  self.quickdraw_data_dict[category][category_idx_sketch]\n",
    "        realimage_ary =  self.realworld_data_dict[category][category_idx_real]\n",
    "        \n",
    "        sample = {'realworld': realimage_ary, 'sketch': quickdraw_ary, 'label': label}\n",
    "        if self.transform:\n",
    "            sample['realworld'] = self.transform_img(sample['realworld'])\n",
    "            sample['sketch'] = self.transform_img(sample['sketch'])\n",
    "            sample['label'] = self.transform_label(sample['label'])\n",
    "        return sample\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9b62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QURIES = ['banana', 'strawberry', 'pear', 'watermelon', 'grapes','pineapple', 'apple', ]  \n",
    "\n",
    "train_quickdraw_dir = './data/compressed_quickdraw_train.npz'\n",
    "test_quickdraw_dir = './data/compressed_quickdraw_test.npz'\n",
    "train_realworld_dir = './data/compressed_realworld_train.npz'\n",
    "test_realworld_dir = './data/compressed_realworld_test.npz'\n",
    "\n",
    "train_both = TwoImageDataset(train_quickdraw_dir, train_realworld_dir, QURIES, image_size = 255, class_size = 4000)\n",
    "test_both = TwoImageDataset(test_quickdraw_dir, test_realworld_dir, QURIES, image_size = 255, class_size = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee95f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbn(in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=7, dropout=0.1):\n",
    "        super(CNN, self).__init__()\n",
    "        layer1   = convbn(n_channels, 64, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer2   = convbn(64,  128, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3   = convbn(128, 192, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer4   = convbn(192, 256, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer1_2 = convbn(64,  64,  kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer2_2 = convbn(128, 128, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer3_2 = convbn(192, 192, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer4_2 = convbn(256, 256, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.layers = nn.Sequential(layer1, layer1_2, layer2, layer2_2, layer3, layer3_2, layer4, layer4_2, pool)\n",
    "        self.nn = nn.Linear(256, n_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "        return x, feats\n",
    "    \n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=7, dropout=0.1):\n",
    "        super(CNN2, self).__init__()\n",
    "        layer1   = convbn(n_channels, 64, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer2   = convbn(64,  128, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3   = convbn(128, 256, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer1_2 = convbn(64,  64,  kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer2_2 = convbn(128, 128, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer3_2 = convbn(256, 256, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.layers = nn.Sequential(layer1, layer1_2, layer2, layer2_2, layer3, layer3_2, pool)\n",
    "        self.nn = nn.Linear(256, n_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "        return x, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adda15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(features, labels, temperature):\n",
    "    \n",
    "    # compute feature\n",
    "    feats_matrix = features.unsqueeze(2).expand(-1, -1, features.size(0)) # (batchsize, feats.len, batchsize)\n",
    "    trans_feats_matrix = feats_matrix.transpose(0, 2)\n",
    "    sim_matrix = F.cosine_similarity(feats_matrix, trans_feats_matrix, dim=1)\n",
    "    \n",
    "    # compute label (batchsize)\n",
    "    batchsize = labels.shape[0]\n",
    "    label_matrix = labels.unsqueeze(-1).expand((batchsize, batchsize))\n",
    "    trans_label_matrix = label_matrix.transpose(0, 1)\n",
    "    target_matrix = (label_matrix == trans_label_matrix).type(torch.float)\n",
    "    \n",
    "    KL = nn.KLDivLoss(reduction=\"batchmean\", log_target=False)\n",
    "    loss = KL(F.softmax(sim_matrix / temperature).log(), F.softmax(target_matrix / temperature))  \n",
    "    return loss\n",
    "\n",
    "\n",
    "class CNNCL(nn.Module):\n",
    "    def __init__(self, n_classes=7, dropout=0.1, t=0.1):\n",
    "        super(CNNCL, self).__init__()\n",
    "        self.t = t\n",
    "        self.quickdraw_model = CNN()\n",
    "        self.realworld_model = CNN2()\n",
    "    def forward(self, quickdraw_x, realworld_x):\n",
    "        quickdraw_pred, quickdraw_feat = self.quickdraw_model(quickdraw_x)\n",
    "        realworld_pred, realworld_feat = self.realworld_model(realworld_x)\n",
    "        return quickdraw_pred, quickdraw_feat, realworld_pred, realworld_feat\n",
    "    def loss(self, quickdraw_pred, quickdraw_feat, realworld_pred, realworld_feat, y):\n",
    "        CE = nn.CrossEntropyLoss()\n",
    "        quickdraw_CE = CE(quickdraw_pred, y)\n",
    "        realworld_CE = CE(realworld_pred, y)\n",
    "        label = y.argmax(1)\n",
    "        CL_quickdraw = contrastive_loss(quickdraw_feat, label, self.t)\n",
    "        CL_realworld = contrastive_loss(realworld_feat, label, self.t)\n",
    "        CL_both = contrastive_loss(quickdraw_feat*realworld_feat, label, self.t)\n",
    "        loss = quickdraw_CE + realworld_CE + CL_quickdraw + CL_realworld + CL_both\n",
    "        return loss, quickdraw_CE, realworld_CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185f79ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CNN_CL = CNNCL().to(device)\n",
    "CNN_CL.load_state_dict(torch.load('./model/CNNCL.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "realworldloader = DataLoader(test_both, batch_size= 100, shuffle=True, pin_memory=True, num_workers=32)\n",
    "CNN_CL.eval()\n",
    "realworld_pred_lst = []\n",
    "realworld_lst = []\n",
    "realworld_y_lst = []\n",
    "with torch.no_grad():\n",
    "     for _, data in enumerate(realworldloader):\n",
    "        realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "        pred1, feat1, pred2, feat2 = CNN_CL(sketch, realworld)\n",
    "        realworld_pred_lst.append(pred2)\n",
    "        realworld_y_lst.append(y.squeeze())\n",
    "        realworld_lst.append(realworld)\n",
    "        if _ == 10:\n",
    "            break\n",
    "realworld_pred = torch.cat(realworld_pred_lst,0)\n",
    "realworld = torch.cat(realworld_lst,0).cpu()\n",
    "\n",
    "class_size = 1000\n",
    "test_quickdraw = dict(np.load('./data/compressed_quickdraw_test.npz'))\n",
    "\n",
    "def search_engine_demo(choose_sketch_index, model, k = 5):\n",
    "    if choose_sketch_index >= 7000:\n",
    "        print('Sorry, we only have 7000 sketch images in test!')\n",
    "        print('Let us pick the 1st sketch image for you to test!')\n",
    "        choose_sketch_index = 1\n",
    "    class_index = int(choose_sketch_index // class_size)\n",
    "    category =  QURIES[class_index]\n",
    "    category_idx = int(choose_sketch_index % class_size)\n",
    "    transform_img = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])   \n",
    "    quickdraw_ary =  transform_img(test_quickdraw[category][category_idx]).unsqueeze(0).cuda()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred1, feat1, pred2, feat2 = model(quickdraw_ary, quickdraw_ary)\n",
    "    quickdraw_y = y.squeeze()\n",
    "    quickdraw_pred = nn.functional.normalize(pred1)\n",
    "    cos_similarity = cos(realworld_pred,quickdraw_pred)\n",
    "    topk_index = torch.topk(cos_similarity, k).indices\n",
    "\n",
    "    topk_index = topk_index.cpu()\n",
    "    return test_quickdraw[category][category_idx], topk_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_search_engine(real_world, choosed_sketch, topk_index):\n",
    "    print('\\n')\n",
    "    print('************** Welcome to Our Search Engine Demo **************')\n",
    "    print('\\n')\n",
    "    print('--------------------- User Input Sketch ----------------------')\n",
    "    fig, axs = plt.subplots(1,1,figsize=(3,3))\n",
    "    axs.imshow(choosed_sketch)\n",
    "    plt.show()\n",
    "\n",
    "    print('------------------ Search Engine Outputs ---------------------')\n",
    "    if len(top_k_index) == 1:\n",
    "        fig_realworld, axs_realworld = plt.subplots(1,1,figsize=(5,5))\n",
    "        axs_realworld.imshow(realworld[top_k_index[0]].permute(1,2,0))\n",
    "    else: \n",
    "        fig_realworld, axs_realworld = plt.subplots(1,len(top_k_index),figsize=(20,80))\n",
    "        for i in range(len(top_k_index)):\n",
    "            k = top_k_index[i]\n",
    "            axs_realworld[i].imshow(realworld[k].permute(1,2,0))\n",
    "    plt.show()\n",
    "    print('************************ THE END *****************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17958b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_sketch_index = 1\n",
    "choosed_sketch, top_k_index = search_engine_demo(choose_sketch_index, CNN_CL, k = 5)\n",
    "plot_search_engine(realworld, choosed_sketch, top_k_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7779b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
