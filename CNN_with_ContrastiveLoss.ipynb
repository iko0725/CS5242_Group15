{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb340e8a",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Contrastive Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf891e2c",
   "metadata": {},
   "source": [
    "Group Member: WuZhanxin\n",
    "\n",
    "In this section, we introduce Contrastive Loss to match the representations between real-world images and sketch images. The formula for the Contrastive Loss we use is in the Report notebook. \n",
    "You can (1) run the training part in the following codes to train a new CNN with Contrastive Loss or (2) directly load my pretrained model.\n",
    "We also provide Search Engine Accuracy Check and Model History Plot here.\n",
    "\n",
    "Enjoy the codes~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee06a4a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca427d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torch_snippets import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d659c45",
   "metadata": {},
   "source": [
    "## Load our Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f40eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "class TwoImageDataset(Dataset):\n",
    "    \"\"\" Own dataset \"\"\"\n",
    "\n",
    "    def __init__(self, quickdraw_dir, realworld_dir, fruit_category, image_size = 255, class_size = 5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sketch_dir (string): Directory to all the sketch images.\n",
    "            realworld_dir (string): Directory to all the real world images.\n",
    "            fruit_category: list to fruit catogory\n",
    "            class_size: Num of images in each category\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.quickdraw_dir = quickdraw_dir\n",
    "        self.realworld_dir = realworld_dir\n",
    "        self.transform = transform\n",
    "        self.fruit_category = fruit_category\n",
    "        self.class_size = class_size\n",
    "        self.quickdraw_data_dict = dict(np.load(quickdraw_dir))\n",
    "        self.realworld_data_dict = dict(np.load(realworld_dir))\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.transform_img = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                transforms.Resize(image_size),\n",
    "                                                transforms.ToTensor()])\n",
    "        self.transform_label = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.class_size * len(self.fruit_category)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        class_index = int(idx // self.class_size)\n",
    "        category =  self.fruit_category[class_index]\n",
    "        category_idx_real = random.choice(range(self.class_size))\n",
    "        category_idx_sketch = random.choice(range(self.class_size))\n",
    "        \n",
    "        label = np.zeros((len(self.fruit_category), 1))\n",
    "        label[class_index] = 1\n",
    "      \n",
    "        quickdraw_ary =  self.quickdraw_data_dict[category][category_idx_sketch]\n",
    "        realimage_ary =  self.realworld_data_dict[category][category_idx_real]\n",
    "        \n",
    "        sample = {'realworld': realimage_ary, 'sketch': quickdraw_ary, 'label': label}\n",
    "        if self.transform:\n",
    "            sample['realworld'] = self.transform_img(sample['realworld'])\n",
    "            sample['sketch'] = self.transform_img(sample['sketch'])\n",
    "            sample['label'] = self.transform_label(sample['label'])\n",
    "        return sample\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564858e",
   "metadata": {},
   "source": [
    "## Load Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbec27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QURIES = ['pineapple', 'apple', 'banana', 'strawberry', 'pear', 'watermelon', 'grapes']  \n",
    "train_quickdraw_dir = './data/compressed_quickdraw_train.npz'\n",
    "test_quickdraw_dir = './data/compressed_quickdraw_test.npz'\n",
    "train_realworld_dir = './data/compressed_realworld_train.npz'\n",
    "test_realworld_dir = './data/compressed_realworld_test.npz'\n",
    "\n",
    "train_both = TwoImageDataset(train_quickdraw_dir, train_realworld_dir, QURIES, image_size = 255, class_size = 4000)\n",
    "test_both = TwoImageDataset(test_quickdraw_dir, test_realworld_dir, QURIES, image_size = 255, class_size = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e7073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_both, batch_size= 128, shuffle=True, pin_memory=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f240df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_both, batch_size= 128, shuffle=True, pin_memory=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05916475",
   "metadata": {},
   "source": [
    "## CNN Model Structure\n",
    "\n",
    "Notice: we use two CNNs for real-world images classification and sketch images classification. Then, the CNNCL combines them together with contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf42bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbn(in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=7, dropout=0.1):\n",
    "        super(CNN, self).__init__()\n",
    "        layer1   = convbn(n_channels, 64, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer2   = convbn(64,  128, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3   = convbn(128, 192, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer4   = convbn(192, 256, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer1_2 = convbn(64,  64,  kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer2_2 = convbn(128, 128, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer3_2 = convbn(192, 192, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer4_2 = convbn(256, 256, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.layers = nn.Sequential(layer1, layer1_2, layer2, layer2_2, layer3, layer3_2, layer4, layer4_2, pool)\n",
    "        self.nn = nn.Linear(256, n_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "        return x, feats\n",
    "    \n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=7, dropout=0.1):\n",
    "        super(CNN2, self).__init__()\n",
    "        layer1   = convbn(n_channels, 64, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer2   = convbn(64,  128, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer3   = convbn(128, 256, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "        layer1_2 = convbn(64,  64,  kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer2_2 = convbn(128, 128, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        layer3_2 = convbn(256, 256, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.layers = nn.Sequential(layer1, layer1_2, layer2, layer2_2, layer3, layer3_2, pool)\n",
    "        self.nn = nn.Linear(256, n_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x):\n",
    "        feats = self.layers(x).flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "        return x, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1677b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(features, labels, temperature):\n",
    "    \n",
    "    # compute feature\n",
    "    feats_matrix = features.unsqueeze(2).expand(-1, -1, features.size(0)) # (batchsize, feats.len, batchsize)\n",
    "    trans_feats_matrix = feats_matrix.transpose(0, 2)\n",
    "    sim_matrix = F.cosine_similarity(feats_matrix, trans_feats_matrix, dim=1)\n",
    "    \n",
    "    # compute label (batchsize)\n",
    "    batchsize = labels.shape[0]\n",
    "    label_matrix = labels.unsqueeze(-1).expand((batchsize, batchsize))\n",
    "    trans_label_matrix = label_matrix.transpose(0, 1)\n",
    "    target_matrix = (label_matrix == trans_label_matrix).type(torch.float)\n",
    "    \n",
    "    KL = nn.KLDivLoss(reduction=\"batchmean\", log_target=False)\n",
    "    loss = KL(F.softmax(sim_matrix / temperature).log(), F.softmax(target_matrix / temperature))  \n",
    "    return loss\n",
    "\n",
    "\n",
    "class CNNCL(nn.Module):\n",
    "    def __init__(self, n_classes=7, dropout=0.1, t=0.1):\n",
    "        super(CNNCL, self).__init__()\n",
    "        self.t = t\n",
    "        self.quickdraw_model = CNN()\n",
    "        self.realworld_model = CNN2()\n",
    "    def forward(self, quickdraw_x, realworld_x):\n",
    "        quickdraw_pred, quickdraw_feat = self.quickdraw_model(quickdraw_x)\n",
    "        realworld_pred, realworld_feat = self.realworld_model(realworld_x)\n",
    "        return quickdraw_pred, quickdraw_feat, realworld_pred, realworld_feat\n",
    "    def loss(self, quickdraw_pred, quickdraw_feat, realworld_pred, realworld_feat, y):\n",
    "        CE = nn.CrossEntropyLoss()\n",
    "        quickdraw_CE = CE(quickdraw_pred, y)\n",
    "        realworld_CE = CE(realworld_pred, y)\n",
    "        label = y.argmax(1)\n",
    "        CL_quickdraw = contrastive_loss(quickdraw_feat, label, self.t)\n",
    "        CL_realworld = contrastive_loss(realworld_feat, label, self.t)\n",
    "        CL_both = contrastive_loss(quickdraw_feat*realworld_feat, label, self.t)\n",
    "        loss = quickdraw_CE + realworld_CE + CL_quickdraw + CL_realworld + CL_both\n",
    "        return loss, quickdraw_CE, realworld_CE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89508451",
   "metadata": {},
   "source": [
    "## Define Train and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f74238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    num_batches = len(dataloader)\n",
    "    correct, quickdraw_correct, realworld_correct = 0, 0, 0\n",
    "    train_loss, quickdraw_loss, realworld_loss = 0, 0, 0\n",
    "    for _, data in enumerate(dataloader):\n",
    "        realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "\n",
    "        pred1, feat1, pred2, feat2 = model(sketch, realworld)\n",
    "        loss, CE_quickdraw, CE_realworld = model.loss(pred1, feat1, pred2, feat2, y.squeeze())\n",
    "        \n",
    "        \n",
    "        label = y.squeeze().argmax(1)\n",
    "        pred_acc1 = (pred1.argmax(1) == label).type(torch.float)\n",
    "        pred_acc2 = (pred2.argmax(1) == label).type(torch.float)\n",
    "        \n",
    "        quickdraw_correct += pred_acc1.sum().item()\n",
    "        realworld_correct += pred_acc2.sum().item()\n",
    "        correct += (pred_acc1 == pred_acc2).type(torch.float).sum().item()\n",
    "        \n",
    "        quickdraw_loss += CE_quickdraw.item()\n",
    "        realworld_loss += CE_realworld.item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if _ % 100 == 0:\n",
    "            loss, current = loss.item(), _ * len(pred1)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    quickdraw_loss /= num_batches\n",
    "    realworld_loss /= num_batches\n",
    "    quickdraw_correct /= size\n",
    "    realworld_correct /= size\n",
    "    correct /= size\n",
    "    \n",
    "    return  correct, quickdraw_correct, realworld_correct, train_loss, quickdraw_loss, realworld_loss\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, quickdraw_loss, realworld_loss, correct, realworld_correct, quickdraw_correct = 0, 0, 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "         for _, data in enumerate(dataloader):\n",
    "            realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "            pred1, feat1, pred2, feat2 = model(sketch, realworld)\n",
    "            label = y.squeeze().argmax(1)\n",
    "            pred_acc1 = (pred1.argmax(1) == label).type(torch.float)\n",
    "            pred_acc2 = (pred2.argmax(1) == label).type(torch.float)\n",
    "            \n",
    "            quickdraw_correct += pred_acc1.sum().item()\n",
    "            realworld_correct += pred_acc2.sum().item()\n",
    "            correct += (pred_acc1 == pred_acc2).type(torch.float).sum().item()\n",
    "            \n",
    "            CL_loss, CE_quickdraw, CE_realworld = model.loss(pred1, feat1, pred2, feat2, y.squeeze())\n",
    "            test_loss += CL_loss.item()\n",
    "            quickdraw_loss += CE_quickdraw.item()\n",
    "            realworld_loss += CE_realworld.item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    quickdraw_loss /= num_batches\n",
    "    realworld_loss /= num_batches\n",
    "    quickdraw_correct /= size\n",
    "    realworld_correct /= size\n",
    "    correct /= size\n",
    "    print(\"------------ Test -----------\")\n",
    "    print(f\"Search Engine Accuracy: {(100*correct):>2f}%, Avg loss: {test_loss:>8f}\")\n",
    "    print(f\"Realworld Accuracy: {(100*realworld_correct):>2f}%\")\n",
    "    print(f\"Quickdraw Accuracy: {(100*quickdraw_correct):>2f}%\\n\")\n",
    "    return correct, quickdraw_correct, realworld_correct, test_loss, quickdraw_loss, realworld_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec0291",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7920b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CNN_CL = CNNCL().to(device)\n",
    "\n",
    "CNN_CL_epochs = 20\n",
    "CNN_CL_optimizer = torch.optim.AdamW(CNN_CL.parameters(), lr= 2e-5, weight_decay=1e-4)\n",
    "CNN_CL_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(CNN_CL_optimizer, T_max=CNN_CL_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ece33b",
   "metadata": {},
   "source": [
    "## Train and Test Procedure\n",
    "\n",
    "Notice: If you want to load our pretrained model, please skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af25d8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ----------\n",
      "Loss: 9.928162  [    0/28000]\n",
      "Loss: 7.136716  [12800/28000]\n",
      "Loss: 6.572839  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 52.700000%, Avg loss: 6.528894\n",
      "Realworld Accuracy: 66.200000%\n",
      "Quickdraw Accuracy: 58.871429%\n",
      "\n",
      "---------- Epoch 2 ----------\n",
      "Loss: 6.580307  [    0/28000]\n",
      "Loss: 6.278971  [12800/28000]\n",
      "Loss: 5.516556  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 63.271429%, Avg loss: 5.428853\n",
      "Realworld Accuracy: 75.642857%\n",
      "Quickdraw Accuracy: 76.657143%\n",
      "\n",
      "---------- Epoch 3 ----------\n",
      "Loss: 5.462167  [    0/28000]\n",
      "Loss: 5.256740  [12800/28000]\n",
      "Loss: 5.331054  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 65.000000%, Avg loss: 4.895337\n",
      "Realworld Accuracy: 74.342857%\n",
      "Quickdraw Accuracy: 81.771429%\n",
      "\n",
      "---------- Epoch 4 ----------\n",
      "Loss: 4.824838  [    0/28000]\n",
      "Loss: 5.105010  [12800/28000]\n",
      "Loss: 4.374037  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 70.100000%, Avg loss: 4.473513\n",
      "Realworld Accuracy: 79.771429%\n",
      "Quickdraw Accuracy: 84.242857%\n",
      "\n",
      "---------- Epoch 5 ----------\n",
      "Loss: 4.659435  [    0/28000]\n",
      "Loss: 4.490874  [12800/28000]\n",
      "Loss: 4.038637  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 71.914286%, Avg loss: 4.117792\n",
      "Realworld Accuracy: 80.028571%\n",
      "Quickdraw Accuracy: 86.542857%\n",
      "\n",
      "---------- Epoch 6 ----------\n",
      "Loss: 4.676816  [    0/28000]\n",
      "Loss: 4.047290  [12800/28000]\n",
      "Loss: 4.500313  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 74.557143%, Avg loss: 3.867704\n",
      "Realworld Accuracy: 82.885714%\n",
      "Quickdraw Accuracy: 87.214286%\n",
      "\n",
      "---------- Epoch 7 ----------\n",
      "Loss: 3.949286  [    0/28000]\n",
      "Loss: 3.748579  [12800/28000]\n",
      "Loss: 3.947062  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 73.485714%, Avg loss: 3.839962\n",
      "Realworld Accuracy: 84.471429%\n",
      "Quickdraw Accuracy: 81.585714%\n",
      "\n",
      "---------- Epoch 8 ----------\n",
      "Loss: 3.766203  [    0/28000]\n",
      "Loss: 3.742229  [12800/28000]\n",
      "Loss: 3.514998  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 75.514286%, Avg loss: 3.545987\n",
      "Realworld Accuracy: 86.414286%\n",
      "Quickdraw Accuracy: 85.528571%\n",
      "\n",
      "---------- Epoch 9 ----------\n",
      "Loss: 3.801611  [    0/28000]\n",
      "Loss: 3.311620  [12800/28000]\n",
      "Loss: 3.583581  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 78.514286%, Avg loss: 3.240909\n",
      "Realworld Accuracy: 86.542857%\n",
      "Quickdraw Accuracy: 89.485714%\n",
      "\n",
      "---------- Epoch 10 ----------\n",
      "Loss: 3.923533  [    0/28000]\n",
      "Loss: 3.056843  [12800/28000]\n",
      "Loss: 3.211416  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 78.114286%, Avg loss: 3.266749\n",
      "Realworld Accuracy: 88.485714%\n",
      "Quickdraw Accuracy: 86.628571%\n",
      "\n",
      "---------- Epoch 11 ----------\n",
      "Loss: 3.639512  [    0/28000]\n",
      "Loss: 3.601894  [12800/28000]\n",
      "Loss: 3.161944  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 80.428571%, Avg loss: 3.105706\n",
      "Realworld Accuracy: 88.185714%\n",
      "Quickdraw Accuracy: 89.814286%\n",
      "\n",
      "---------- Epoch 12 ----------\n",
      "Loss: 3.555663  [    0/28000]\n",
      "Loss: 2.977780  [12800/28000]\n",
      "Loss: 3.152656  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 81.371429%, Avg loss: 3.076043\n",
      "Realworld Accuracy: 89.157143%\n",
      "Quickdraw Accuracy: 89.585714%\n",
      "\n",
      "---------- Epoch 13 ----------\n",
      "Loss: 3.364204  [    0/28000]\n",
      "Loss: 2.908645  [12800/28000]\n",
      "Loss: 2.718882  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 81.057143%, Avg loss: 2.958277\n",
      "Realworld Accuracy: 88.585714%\n",
      "Quickdraw Accuracy: 90.385714%\n",
      "\n",
      "---------- Epoch 14 ----------\n",
      "Loss: 2.849030  [    0/28000]\n",
      "Loss: 2.863097  [12800/28000]\n",
      "Loss: 2.594279  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 80.657143%, Avg loss: 2.967605\n",
      "Realworld Accuracy: 88.757143%\n",
      "Quickdraw Accuracy: 89.671429%\n",
      "\n",
      "---------- Epoch 15 ----------\n",
      "Loss: 3.061746  [    0/28000]\n",
      "Loss: 2.524107  [12800/28000]\n",
      "Loss: 2.950979  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 81.057143%, Avg loss: 2.892465\n",
      "Realworld Accuracy: 88.528571%\n",
      "Quickdraw Accuracy: 90.157143%\n",
      "\n",
      "---------- Epoch 16 ----------\n",
      "Loss: 3.302429  [    0/28000]\n",
      "Loss: 3.232336  [12800/28000]\n",
      "Loss: 2.865105  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 80.471429%, Avg loss: 2.878167\n",
      "Realworld Accuracy: 88.742857%\n",
      "Quickdraw Accuracy: 89.271429%\n",
      "\n",
      "---------- Epoch 17 ----------\n",
      "Loss: 2.502051  [    0/28000]\n",
      "Loss: 2.897314  [12800/28000]\n",
      "Loss: 2.575834  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 82.157143%, Avg loss: 2.793540\n",
      "Realworld Accuracy: 90.028571%\n",
      "Quickdraw Accuracy: 89.928571%\n",
      "\n",
      "---------- Epoch 18 ----------\n",
      "Loss: 2.970635  [    0/28000]\n",
      "Loss: 2.823990  [12800/28000]\n",
      "Loss: 2.721898  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 81.985714%, Avg loss: 2.813662\n",
      "Realworld Accuracy: 89.385714%\n",
      "Quickdraw Accuracy: 90.514286%\n",
      "\n",
      "---------- Epoch 19 ----------\n",
      "Loss: 3.135628  [    0/28000]\n",
      "Loss: 2.989845  [12800/28000]\n",
      "Loss: 2.361028  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 82.242857%, Avg loss: 2.808876\n",
      "Realworld Accuracy: 89.685714%\n",
      "Quickdraw Accuracy: 90.671429%\n",
      "\n",
      "---------- Epoch 20 ----------\n",
      "Loss: 2.557800  [    0/28000]\n",
      "Loss: 2.850976  [12800/28000]\n",
      "Loss: 2.865815  [25600/28000]\n",
      "------------ Test -----------\n",
      "Search Engine Accuracy: 81.771429%, Avg loss: 2.805535\n",
      "Realworld Accuracy: 90.071429%\n",
      "Quickdraw Accuracy: 89.757143%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_accuracy = []\n",
    "test_quickdraw_accuracy = []\n",
    "test_realworld_accuracy = []\n",
    "test_quickdraw_loss = []\n",
    "test_realworld_loss = []\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "train_quickdraw_accuracy = []\n",
    "train_realworld_accuracy = []\n",
    "train_quickdraw_loss = []\n",
    "train_realworld_loss = []\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "for t in range(CNN_CL_epochs):\n",
    "    print(f\"---------- Epoch {t+1} ----------\")\n",
    "    train_acc, train_q_acc, train_r_acc, train_loss, train_q_loss, train_r_loss = train(trainloader, CNN_CL, CNN_CL_optimizer)\n",
    "    test_acc, test_q_acc, test_r_acc, test_loss, test_q_loss, test_r_loss = test(testloader, CNN_CL)\n",
    "    CNN_CL_scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "    train_quickdraw_accuracy.append(train_q_acc)\n",
    "    train_realworld_accuracy.append(train_r_acc)\n",
    "    train_quickdraw_loss.append(train_q_loss)\n",
    "    train_realworld_loss.append(train_r_loss)\n",
    "\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracy.append(test_acc)\n",
    "    test_quickdraw_accuracy.append(test_q_acc)\n",
    "    test_realworld_accuracy.append(test_r_acc)\n",
    "    test_quickdraw_loss.append(test_q_loss)\n",
    "    test_realworld_loss.append(test_r_loss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5b9b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Creating Real World Image Feature!\n",
      "Find Top 1 Realworld Images Retreive Accuracy: 75.599998%\n",
      "Find Top 3 Realworld Images Retreive Accuracy: 81.690475%\n",
      "Find Top 5 Realworld Images Retreive Accuracy: 84.057144%\n",
      "Find Top 10 Realworld Images Retreive Accuracy: 85.868576%\n",
      "Find Top 20 Realworld Images Retreive Accuracy: 85.303566%\n"
     ]
    }
   ],
   "source": [
    "realworldloader = DataLoader(test_both, batch_size= 100, shuffle=True, pin_memory=True, num_workers=32)\n",
    "CNN_CL.eval()\n",
    "realworld_pred_lst = []\n",
    "realworld_y_lst = []\n",
    "with torch.no_grad():\n",
    "     for _, data in enumerate(realworldloader):\n",
    "        realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "        pred1, feat1, pred2, feat2 = CNN_CL(sketch, realworld)\n",
    "        realworld_pred_lst.append(pred2)\n",
    "        realworld_y_lst.append(y.squeeze())\n",
    "realworld_pred = torch.cat(realworld_pred_lst,0)\n",
    "realworld_y = torch.cat(realworld_y_lst,0)\n",
    "print('Finish Creating Real World Image Feature!')\n",
    "\n",
    "retrieve_quickdraw_loader = DataLoader(test_both, batch_size= 1, shuffle=True, pin_memory=True, num_workers=32)\n",
    "\n",
    "def search_engine_evaluation(retrieve_quickdraw_loader, realworld_pred, k = 3):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    count_accuracy = 0\n",
    "    CNN_CL.eval()\n",
    "    with torch.no_grad():\n",
    "         for _, data in enumerate(retrieve_quickdraw_loader):\n",
    "            realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "            pred1, feat1, pred2, feat2 = CNN_CL(sketch, realworld)\n",
    "            quickdraw_y = y.squeeze()\n",
    "            \n",
    "            realworld_pred = nn.functional.normalize(realworld_pred)\n",
    "            quickdraw_pred = nn.functional.normalize(pred1)\n",
    "            \n",
    "            cos_similarity = cos(realworld_pred,quickdraw_pred)\n",
    "            topk_index = torch.topk(cos_similarity, k).indices\n",
    "            count_accuracy += (realworld_y[topk_index].argmax(1) == quickdraw_y.argmax()).sum()\n",
    "    final_acc = (count_accuracy/(len(retrieve_quickdraw_loader) * k ) )\n",
    "    print(f'Find Top {k} Realworld Images Retreive Accuracy: {(100*final_acc):>2f}%')\n",
    "    return final_acc\n",
    "\n",
    "test_k = [1,3,5,10,20]\n",
    "topk_acc = []\n",
    "for k in test_k:\n",
    "    k_acc = search_engine_evaluation(retrieve_quickdraw_loader, realworld_pred, k)\n",
    "    topk_acc.append(k_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f813b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNN_CL.state_dict(), './model/CNNCL.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de284d",
   "metadata": {},
   "source": [
    "## Load our Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae75c28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_CL = CNNCL().to(device)\n",
    "CNN_CL.load_state_dict(torch.load('./model/CNNCL.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94f44b",
   "metadata": {},
   "source": [
    "## Test Search Engine Accuracy\n",
    "\n",
    "The test_k includes all the k value we want to test, where k means the number of real-world images we will return for each sketch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb3cf957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Creating Real World Image Feature!\n",
      "Find Top 1 Realworld Images Retreive Accuracy: 78.742859%\n",
      "Find Top 3 Realworld Images Retreive Accuracy: 82.728569%\n",
      "Find Top 5 Realworld Images Retreive Accuracy: 83.165718%\n",
      "Find Top 10 Realworld Images Retreive Accuracy: 85.301430%\n",
      "Find Top 20 Realworld Images Retreive Accuracy: 83.523575%\n"
     ]
    }
   ],
   "source": [
    "realworldloader = DataLoader(test_both, batch_size= 100, shuffle=True, pin_memory=True, num_workers=32)\n",
    "CNN_CL.eval()\n",
    "realworld_pred_lst = []\n",
    "realworld_y_lst = []\n",
    "with torch.no_grad():\n",
    "     for _, data in enumerate(realworldloader):\n",
    "        realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "        pred1, feat1, pred2, feat2 = CNN_CL(sketch, realworld)\n",
    "        realworld_pred_lst.append(pred2)\n",
    "        realworld_y_lst.append(y.squeeze())\n",
    "realworld_pred = torch.cat(realworld_pred_lst,0)\n",
    "realworld_y = torch.cat(realworld_y_lst,0)\n",
    "print('Finish Creating Real World Image Feature!')\n",
    "\n",
    "retrieve_quickdraw_loader = DataLoader(test_both, batch_size= 1, shuffle=True, pin_memory=True, num_workers=32)\n",
    "\n",
    "def search_engine_evaluation(retrieve_quickdraw_loader, realworld_pred, k = 3):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    count_accuracy = 0\n",
    "    CNN_CL.eval()\n",
    "    with torch.no_grad():\n",
    "         for _, data in enumerate(retrieve_quickdraw_loader):\n",
    "            realworld, sketch, y = data['realworld'].to(device), data['sketch'].to(device), data['label'].to(device)\n",
    "            pred1, feat1, pred2, feat2 = CNN_CL(sketch, realworld)\n",
    "            quickdraw_y = y.squeeze()\n",
    "            \n",
    "            realworld_pred = nn.functional.normalize(realworld_pred)\n",
    "            quickdraw_pred = nn.functional.normalize(pred1)\n",
    "            \n",
    "            cos_similarity = cos(realworld_pred,quickdraw_pred)\n",
    "            topk_index = torch.topk(cos_similarity, k).indices\n",
    "            count_accuracy += (realworld_y[topk_index].argmax(1) == quickdraw_y.argmax()).sum()\n",
    "    final_acc = (count_accuracy/(len(retrieve_quickdraw_loader) * k ) )\n",
    "    print(f'Find Top {k} Realworld Images Retreive Accuracy: {(100*final_acc):>2f}%')\n",
    "    return final_acc\n",
    "\n",
    "test_k = [1,3,5,10,20]\n",
    "topk_acc = []\n",
    "for k in test_k:\n",
    "    k_acc = search_engine_evaluation(retrieve_quickdraw_loader, realworld_pred, k)\n",
    "    topk_acc.append(k_acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979b561",
   "metadata": {},
   "source": [
    "## Save all the results \n",
    "\n",
    "### Important Notice: only run the following codes when you retrain the model; otherwise, it will cover our submitted parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e258a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_history = {\n",
    "    \"epoch\": range(len(train_losses)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"train_acc\": train_accuracy,\n",
    "    \"val_loss\": test_losses,\n",
    "    \"val_acc\": test_accuracy,\n",
    "\n",
    "    # Please save the following metric\n",
    "    \"train_realworld_acc\": train_realworld_accuracy,\n",
    "    \"train_quickdraw_acc\": train_quickdraw_accuracy,\n",
    "    \"val_realworld_acc\": test_realworld_accuracy,\n",
    "    \"val_quickdraw_acc\": test_quickdraw_accuracy,\n",
    "    \"train_realworld_loss\": train_realworld_loss,\n",
    "    \"train_quickdraw_loss\": train_quickdraw_loss,\n",
    "    \"val_realworld_loss\": test_realworld_loss,\n",
    "    \"val_quickdraw_loss\": test_quickdraw_loss,\n",
    "    \"k_choice\": test_k, # [1,3,5,10,20]\n",
    "    \"topk_acc\": topk_acc\n",
    "}\n",
    "\n",
    "\n",
    "def save_model_history(metrics, filepath):\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(metrics, file)\n",
    "\n",
    "def load_model_history(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        metrics = pickle.load(file)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_history(model_history, './results/CNNCL.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed50875",
   "metadata": {},
   "source": [
    "## Load Our Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418e403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history = load_model_history('./results/CNNCL.pkl')\n",
    "model_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0d6c2",
   "metadata": {},
   "source": [
    "## Plot Our Model History\n",
    "\n",
    "Notice: for more information and plot for the model history, please check the Report notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96198679",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda30bac6a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(range(len(train_losses)), train_losses, label = ' training loss')\n",
    "# plt.plot(range(len(test_losses)), test_losses, label = 'test loss')\n",
    "# train_accuracy= model_history['train_acc']\n",
    "# test_accuracy =  model_history['train_acc']\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, label = 'train_accuracy')\n",
    "plt.plot(range(len(test_accuracy)), test_accuracy, label = 'val_accuracy')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6881389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e0a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe39663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1f65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
